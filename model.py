# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzZ27Mqq2ujolwsWeDOMq7uTEygK320J
"""

import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2  # adjustable

class BpeDataset(Dataset):
    def __init__(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        item = self.data[idx]
        src = torch.tensor(item['urdu_ids'], dtype=torch.long)
        trg = torch.tensor(item['roman_ids'], dtype=torch.long)
        return src, trg

def collate_fn(batch):
    srcs, trgs = zip(*batch)
    src_lens = torch.tensor([len(s) for s in srcs])
    trg_lens = torch.tensor([len(t) for t in trgs])
    src_pad = pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)
    trg_pad = pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)
    return src_pad, src_lens, trg_pad, trg_lens

# Paths for Colab
train_ds = BpeDataset("train_bpe.json")
val_ds   = BpeDataset("val_bpe.json")
test_ds  = BpeDataset("test_bpe.json")

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)
val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)


# Loss function (ignores PAD tokens during training)
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

#for checking only
src, src_lens, trg, trg_lens = next(iter(train_loader))
print("SRC batch:", src.shape)
print("TRG batch:", trg.shape)
print("SRC lens:", src_lens[:10])
print("TRG lens:", trg_lens[:10])

# Find actual vocab sizes from dataset
max_src_id = max(max(item['urdu_ids']) for item in train_ds.data)
max_trg_id = max(max(item['roman_ids']) for item in train_ds.data)

SRC_VOCAB_SIZE = max_src_id + 1
TRG_VOCAB_SIZE = max_trg_id + 1

print("SRC vocab size:", SRC_VOCAB_SIZE)
print("TRG vocab size:", TRG_VOCAB_SIZE)

# Sanity check batch
src_pad, src_lens, trg_pad, trg_lens = next(iter(train_loader))
print("SRC shape:", src_pad.shape)
print("TRG shape:", trg_pad.shape)

# Create fake logits with correct vocab size
logits = torch.randn(trg_pad.size(0), trg_pad.size(1), TRG_VOCAB_SIZE)

# Compute loss ignoring PAD_IDX
out_dim = logits.shape[-1]
loss = criterion(logits.reshape(-1, out_dim), trg_pad.reshape(-1))
print("Sample loss:", loss.item())

class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        # encoder is bidirectional → enc_hid_dim*2
        self.enc_proj = nn.Linear(enc_hid_dim*2, dec_hid_dim)
        self.attn = nn.Linear(dec_hid_dim, dec_hid_dim)

    def forward(self, hidden, encoder_outputs, mask=None):
        # encoder_outputs: [batch, src_len, enc_hid*2]
        # hidden: [batch, dec_hid]

        # project encoder outputs → [batch, src_len, dec_hid]
        proj_enc = self.enc_proj(encoder_outputs)  # same dtype as encoder_outputs

        # transform hidden → [batch, dec_hid, 1]
        query = self.attn(hidden).unsqueeze(2)

        # scores: [batch, src_len]
        scores = torch.bmm(proj_enc, query).squeeze(2)

        if mask is not None:
            # ensure boolean mask on same device
            mask_bool = mask.to(dtype=torch.bool, device=scores.device)
            # use min representable value for the current dtype (safe for float16/32)
            neg_inf = torch.finfo(scores.dtype).min / 2  # divide to avoid potential extreme underflow
            scores = scores.masked_fill(~mask_bool, neg_inf)

        attn_weights = torch.softmax(scores, dim=1)
        # context: weighted sum over projected encoder outputs (not raw enc_out)
        context = torch.bmm(attn_weights.unsqueeze(1), proj_enc).squeeze(1)  # [batch, dec_hid]

        return context, attn_weights

import json, math, torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from torch.optim import Adam
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import sacrebleu

# ---- constants ----
PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ---- Dataset loader (expects /content/train_bpe.json etc) ----
class BpeDataset(Dataset):
    def __init__(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
    def __len__(self): return len(self.data)
    def __getitem__(self, idx):
        it = self.data[idx]
        src = torch.tensor(it['urdu_ids'], dtype=torch.long)
        trg = torch.tensor(it['roman_ids'], dtype=torch.long)
        return src, trg

def collate_fn(batch):
    srcs, trgs = zip(*batch)
    src_lens = torch.tensor([len(s) for s in srcs], dtype=torch.long)
    trg_lens = torch.tensor([len(t) for t in trgs], dtype=torch.long)
    src_pad = pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)
    trg_pad = pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)
    return src_pad, src_lens, trg_pad, trg_lens

train_ds = BpeDataset("train_bpe.json")
val_ds   = BpeDataset("val_bpe.json")
test_ds  = BpeDataset("test_bpe.json")

BATCH = 64
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, collate_fn=collate_fn)
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, collate_fn=collate_fn)

# ---- utilities: build vocab sizes and id->token maps from your JSON files ----
def get_global_vocab_size(paths, id_key):
    max_id = -1
    for path in paths:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for item in data:
            if item[id_key]:
                max_id = max(max_id, max(item[id_key]))
    return max_id + 1



# ---- load vocab files instead of JSON ----
def load_vocab(path):
    id2tok = {}
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            token = line.strip().split()[0]   # works for both BPE and WordPiece vocab
            id2tok[str(i)] = token
    # ensure special tokens
    id2tok[str(PAD_IDX)] = "<PAD>"
    id2tok[str(SOS_IDX)] = "<SOS>"
    id2tok[str(EOS_IDX)] = "<EOS>"
    return id2tok

src_id2tok = load_vocab("urdu_bpe.vocab")
tgt_id2tok = load_vocab("roman_wp-vocab.txt")


OUTPUT_DIM = get_global_vocab_size(
    ["train_bpe.json", "val_bpe.json", "test_bpe.json"],
    "roman_ids"
)
INPUT_DIM = get_global_vocab_size(
    ["train_bpe.json", "val_bpe.json", "test_bpe.json"],
    "urdu_ids"
)
print("SRC vocab size:", INPUT_DIM)
print("TRG vocab size:", OUTPUT_DIM)

# ---- Model definitions (BiLSTM encoder 2 layers, LSTM decoder 4 layers) ----
class EncoderBiLSTM(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)
        self.emb_dropout = nn.Dropout(0.2)
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,
                            bidirectional=True, batch_first=True, dropout=dropout)
        self.fc_h = nn.Linear(hid_dim*2, hid_dim)
        self.fc_c = nn.Linear(hid_dim*2, hid_dim)
    def forward(self, src, src_len):
        emb = self.emb_dropout(self.embedding(src))                            # [b, src_len, emb]
        packed = pack_padded_sequence(emb, src_len.cpu(), batch_first=True, enforce_sorted=False)
        packed_out, (h, c) = self.lstm(packed)
        enc_out, _ = pad_packed_sequence(packed_out, batch_first=True)
        h_cat = torch.cat((h[-2,:,:], h[-1,:,:]), dim=1)     # [b, hid*2]
        c_cat = torch.cat((c[-2,:,:], c[-1,:,:]), dim=1)
        h0 = torch.tanh(self.fc_h(h_cat)).unsqueeze(0)       # [1,b,hid]
        c0 = torch.tanh(self.fc_c(c_cat)).unsqueeze(0)
        return enc_out, (h0, c0)

class DecoderWithAttention(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)
        self.emb_dropout = nn.Dropout(0.2)
        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.n_layers = n_layers
        self.attention = Attention(hid_dim, hid_dim)

    def forward_step(self, input_token, hidden, cell, encoder_outputs, mask=None):
        emb = self.emb_dropout(self.embedding(input_token)).unsqueeze(1)  # [b,1,emb]
        context, attn_weights = self.attention(hidden[-1], encoder_outputs, mask)  # use last layer hidden
        context = context.unsqueeze(1)  # [b,1,hid]
        rnn_input = torch.cat([emb, context], dim=2)   # [b,1,emb+hid]
        out, (h, c) = self.lstm(rnn_input, (hidden, cell))
        pred = self.fc_out(out.squeeze(1))
        return pred, h, c, attn_weights

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, src_len, trg=None, teacher_forcing=0.5, max_len=100):
        bsz = src.size(0)
        trg_len = trg.size(1) if trg is not None else max_len
        vocab_size = self.decoder.fc_out.out_features
        outputs = torch.zeros(bsz, trg_len, vocab_size, device=self.device)

        # inside Seq2Seq.forward(), before the decode loop:
        enc_out, (h_enc, c_enc) = self.encoder(src, src_len)  # h_enc: [1, b, hid]

        # expand to decoder layers
        if h_enc.size(0) != self.decoder.n_layers:
           h = h_enc.repeat(self.decoder.n_layers, 1, 1).contiguous()
           c = c_enc.repeat(self.decoder.n_layers, 1, 1).contiguous()
        else:
           h, c = h_enc, c_enc

        # attention mask: True where token is NOT PAD
        mask = (src != PAD_IDX).to(self.device)  # [b, src_len]

        input_token = trg[:,0] if trg is not None else torch.full((bsz,), SOS_IDX, dtype=torch.long, device=self.device)

        for t in range(1, trg_len):
            # pass mask into decoder forward step, decoder uses attention(mask)
            pred, h, c, _ = self.decoder.forward_step(input_token, h, c, enc_out, mask=mask)
            outputs[:, t, :] = pred
            if trg is not None:
                if torch.rand(1).item() < teacher_forcing:
                    input_token = trg[:, t]
                else:
                    input_token = pred.argmax(1)
            else:
                input_token = pred.argmax(1)
        return outputs

# 1) Check max index in roman_ids
import json

def check_max_ids(path, key):
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    max_id = max(max(item[key]) for item in data)
    return max_id

print("Max roman id (train):", check_max_ids("train_bpe.json", "roman_ids"))
print("Max roman id (val):", check_max_ids("val_bpe.json", "roman_ids"))
print("Max roman id (test):", check_max_ids("test_bpe.json", "roman_ids"))

print("OUTPUT_DIM:", OUTPUT_DIM)

print("Max roman id (train):", check_max_ids("train_bpe.json", "roman_ids"))
print("Max roman id (val):",   check_max_ids("val_bpe.json", "roman_ids"))
print("Max roman id (test):",  check_max_ids("test_bpe.json", "roman_ids"))
print("TRG vocab size:", OUTPUT_DIM)

# ---- training helpers ----
def ids_to_string(ids, id2tok):
    toks = []
    for idx in ids:
        if idx == PAD_IDX: continue
        if idx == SOS_IDX: continue
        if idx == EOS_IDX: break
        toks.append(id2tok.get(str(idx), "<UNK>"))
    s = "".join(toks).replace(" ", " ").strip()
    return s

def levenshtein(a,b):
    n,m = len(a), len(b)
    if n==0: return m
    dp = list(range(m+1))
    for i in range(1,n+1):
        prev, dp[0] = dp[0], i
        for j in range(1,m+1):
            cur = min(dp[j] + 1, prev + (a[i-1] != b[j-1]), dp[j-1] + 1)
            prev, dp[j] = dp[j], cur
    return dp[m]

def compute_metrics(preds, refs, val_loss):
    bleu = sacrebleu.corpus_bleu(preds, [refs]).score if len(preds)>0 else 0.0
    cers = [levenshtein(p,r)/max(1,len(r)) for p,r in zip(preds, refs)] if len(preds)>0 else [1.0]
    cer = sum(cers)/len(cers)
    try:
        ppl = math.exp(val_loss)
    except OverflowError:
        ppl = float('inf')
    return bleu, cer, ppl

# ---- train / eval functions (AMP-safe) ----
def train_epoch(model, loader, optimizer, criterion, scaler, tf_ratio, scheduler=None):
    model.train()
    total_loss = 0.0
    for src, src_len, trg, _ in tqdm(loader, desc="train", leave=False):
        src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)
        optimizer.zero_grad()
        with autocast():
            outputs = model(src, src_len, trg, teacher_forcing=tf_ratio)
            out_dim = outputs.shape[-1]
            out_len = outputs.size(1)
            trg_len = trg.size(1)
            min_len = min(out_len, trg_len)

            loss = criterion(outputs[:, :min_len, :].reshape(-1, out_dim),trg[:, :min_len].reshape(-1))

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        if scheduler is not None:
            scheduler.step()

        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, criterion, id2tok):
    model.eval()
    total_loss = 0.0
    preds_str = []
    refs_str = []
    with torch.no_grad():
        for src, src_len, trg, _ in tqdm(loader, desc="eval", leave=False):
            src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)
            outputs = model(src, src_len, trg=None, teacher_forcing=0.0, max_len=trg.size(1))
            out_dim = outputs.shape[-1]
            out_len = outputs.size(1)
            trg_len = trg.size(1)
            # Align lengths for loss calculation
            min_len = min(out_len, trg_len)
            loss = criterion(outputs[:, :min_len, :].reshape(-1, out_dim), trg[:, :min_len].reshape(-1))
            total_loss += loss.item()
            top = outputs.argmax(-1).cpu().tolist()
            for i in range(len(top)):
                preds_str.append(ids_to_string(top[i], id2tok))
                refs_str.append(ids_to_string(trg[i].cpu().tolist(), id2tok))
    return total_loss / len(loader), preds_str, refs_str

import matplotlib.pyplot as plt
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR


def run_training(INPUT_DIM, OUTPUT_DIM,
                 emb_dim=256, hid_dim=512,
                 enc_layers=4, dec_layers=4,
                 dropout=0.3, lr=5e-4,
                 epochs=12, save_dir="/",
                 resume_from=None):

    # 1) Build model with attention
    encoder = EncoderBiLSTM(INPUT_DIM, emb_dim, hid_dim,
                            n_layers=enc_layers, dropout=dropout, pad_idx=PAD_IDX)
    decoder = DecoderWithAttention(OUTPUT_DIM, emb_dim, hid_dim,
                                   n_layers=dec_layers, dropout=dropout, pad_idx=PAD_IDX)
    model = Seq2Seq(encoder, decoder, device).to(device)

    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    scheduler = OneCycleLR(
    optimizer,
    max_lr=lr,
    steps_per_epoch=len(train_loader),
    epochs=epochs,
    pct_start=0.05,
    div_factor=25.0,
    final_div_factor=1e4,
    anneal_strategy="cos"
    )

    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.05)

    scaler = GradScaler()   # AMP scaler

    start_epoch = 0
    best_val_loss = float("inf")
    best_epoch = -1

    # Resume from checkpoint if provided
    if resume_from is not None:
        checkpoint = torch.load(resume_from, map_location=device)
        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        scaler.load_state_dict(checkpoint["scaler_state"])
        start_epoch = checkpoint["epoch"] + 1
        best_val_loss = checkpoint.get("best_val_loss", float("inf"))
        best_epoch = checkpoint.get("best_epoch", -1)
        print(f"Resumed training from epoch {start_epoch} using {resume_from}")

    # 2) Track history + scheduler
    history = {"train_loss": [], "val_loss": [], "bleu": [], "cer": [], "ppl": []}

    # 3) Training loop
    for epoch in range(start_epoch, start_epoch + epochs):
        tf = max(0.0, 1 - epoch / max(1, start_epoch + epochs))   # teacher forcing decay

        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, tf, scheduler)
        val_loss, preds, refs = evaluate(model, val_loader, criterion, tgt_id2tok)
        bleu, cer, ppl = compute_metrics(preds, refs, val_loss)

        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["bleu"].append(bleu)
        history["cer"].append(cer)
        history["ppl"].append(ppl)

        print(f"[E{epoch+1}] train_loss={train_loss:.4f} "
              f"val_loss={val_loss:.4f} BLEU={bleu:.2f} CER={cer:.4f} PPL={ppl:.2f}")

        # save full checkpoint
        checkpoint = {
            "epoch": epoch,
            "model_state": model.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "scaler_state": scaler.state_dict(),
            "scheduler_state": scheduler.state_dict(),
            "best_val_loss": best_val_loss,
            "best_epoch": best_epoch
        }
        torch.save(checkpoint, f"{save_dir}/checkpoint_epoch{epoch+1}.pt")

        # save best checkpoint
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch + 1
            torch.save(checkpoint, f"{save_dir}/best_checkpoint1.pt")
            print(f"==> Saved best model at epoch {best_epoch} (val_loss={best_val_loss:.4f})")

        scaler.step(optimizer)
        scaler.update()

    # 4) (Optional) Plotting can go here if you want curves

    return history, model

